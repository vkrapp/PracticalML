{"name":"Practicalml","tagline":"","body":"---\r\ntitle: \"Predicting Exercise Performance with Accelerometer Data\"\r\nauthor: \"vkrapp\"\r\ndate: \"Sunday, April 19, 2015\"\r\noutput: pdf_document\r\n---\r\n\r\n\r\n##Executive summary\r\nIn this project, we use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E).\r\nClass A corresponds to the specified execution of the exercise, while the other 4 classes correspond to common mistakes. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har. The goal of the project was to predict the manner in which they did the exercise. \r\nAfter cleaning the data, the random forest algorithm was applied to build the model.\r\n\r\n## Needed Packages\r\n``` {r Packages}\r\nlibrary(caret)\r\n```\r\n\r\n##Reading and Cleaning Data\r\nFirst, we read the data into R and remove all columns that include empty spaces or NAs, since these columns are not useful for fitting models.The data files are already downloaded and saved in the current working directory.\r\n``` {r GetCleanData}\r\ntrain <- read.csv(\"pml-training.csv\", na.strings = c(\"NA\", \"\"))\r\ntest <-  read.csv(\"pml-testing.csv\", na.strings = c(\"NA\", \"\")) # This data set is for the submission\r\ndim(train) #19622   160 -> large sample\r\ndim(test) #20 160\r\n# All variables that include NAs or empty space are removed:\r\ncomplete <- complete.cases(t(train))\r\ntrain <- train[,complete]\r\ntest <- test[,complete]                   \r\n```\r\nNext, variables that include timing information or the index of the observation are removed because they do not provide relevant information for our model.  \r\n``` {r CleanData}\r\ntrain <- train[,c(-1,-3:-7)]\r\ntest <- test[,c(-1,-3:-7)]\r\ndim(train) #19622    54\r\n```\r\nWith this simplified data set we are going to build our model.\r\n\r\n## Cross Validation and Model Fitting\r\nThe data set is splitted into a training (70%) and a test (validation) set (30%). We perform the random forest algorithm only on the training set with a 5-fold cross-validation. To estimate the out-of-sample-error, we test our model`s accuracy on an independent test set. \r\n``` {r  TrainSplit}\r\nset.seed(2005)\r\ninTrain <- createDataPartition(y=train$classe, p=0.7, list=FALSE)\r\ntraining <- train[inTrain,]\r\ntesting <- train[-inTrain,]\r\n\r\n# Fit model\r\nset.seed(2005)\r\nmodelFit <- train(classe~., method=\"rf\", data=training, \r\n                  trControl = trainControl(method = \"cv\", number = 5))\r\nmodelFit\r\nmodelFit$finalModel\r\n\r\n# Test model on testing data\r\npredictions <- predict(modelFit, testing)\r\n\r\n# Evaluation of prediction\r\nmatrix <- confusionMatrix(predictions,testing$classe)\r\nmatrix\r\n```\r\n\r\nSo with this approach, we get a accuracy of `r matrix$overall[[1]]` on our testing data set. This is a pretty good prediction.   \r\nIn a next step, the ten most important variables for the model are plotted.\r\n```{r Importance}\r\n# Get the variable importance\r\nVarImportance <- varImp(modelFit)\r\n# Plot the 10 most important predictors\r\nplot(VarImportance, main = \"Top 10 of the Most Important Predictors\", top = 10)\r\n\r\n```\r\n\r\n## Submission\r\nIn the end, we use our model to predict the classes of the 20 test cases.\r\n``` {r  Submission}\r\nanswers = predict(modelFit, test)\r\npml_write_files = function(x){\r\n    n = length(x)\r\n    for(i in 1:n){\r\n        filename = paste0(\"problem_id_\",i,\".txt\")\r\n        write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)\r\n        }\r\n    }\r\npml_write_files(answers)\r\n```\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}